{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f430516",
   "metadata": {},
   "source": [
    "# Median Data Preprocessing\n",
    "\n",
    "## Purpose\n",
    "Clean and merge rental property spreadsheets, then enrich suburb data with SA2/LGA regions.\n",
    "\n",
    "## Inputs\n",
    "- `Moving annual median rent by suburb and town - March quarter 2025.xlsx`  \n",
    "- `mapped_target_suburbs.csv`  \n",
    "\n",
    "## Outputs\n",
    "- `all_properties_tidy.csv` – tidy suburb-month metrics  \n",
    "- `all_properties_tidy_enriched.csv` – enriched with SA2/LGA info  \n",
    "- `all_properties_tidy_UNMATCHED_SUBURBS.csv` – unmatched suburbs  \n",
    "\n",
    "## Steps\n",
    "1. Define project paths with `pathlib`.  \n",
    "2. Split Excel sheets → individual CSVs.  \n",
    "3. Strip headers and clean data.  \n",
    "4. Rebuild headers, reshape to tidy format.  \n",
    "5. Combine and export tidy CSV.  \n",
    "6. Join with suburb mapping for SA2/LGA enrichment.  \n",
    "7. Save enriched and unmatched outputs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from dateutil import parser as dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d964680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      "  EXCEL_PATH   = ../../datasets/property/Moving annual median rent by suburb and town - March quarter 2025.xlsx\n",
      "  MAP_CSV      = ../../datasets/6. excel to sa2/mapped_target_suburbs.csv\n",
      "  SPLIT_DIR    = ../../datasets/raw/split_sheets\n",
      "  STRIPPED_DIR = ../../datasets/raw/split_sheets/split_sheets_stripped\n",
      "  TIDY_DIR     = ../../datasets/raw/tidy_monthly\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Project/data root\n",
    "DATA_ROOT = Path(\"../../datasets\")\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "SPLIT_DIR = RAW_DIR / \"split_sheets\"\n",
    "STRIPPED_DIR = SPLIT_DIR / \"split_sheets_stripped\"\n",
    "TIDY_DIR = RAW_DIR / \"tidy_monthly\"\n",
    "\n",
    "# Excel and mapping files\n",
    "EXCEL_PATH = DATA_ROOT / \"property\" / \"Moving annual median rent by suburb and town - March quarter 2025.xlsx\"\n",
    "MAP_CSV = DATA_ROOT / \"6. excel to sa2\" / \"mapped_target_suburbs.csv\"\n",
    "\n",
    "# Output files\n",
    "OUT_TIDY = TIDY_DIR / \"all_properties_tidy.csv\"\n",
    "OUT_ENRICHED = TIDY_DIR / \"all_properties_tidy_enriched.csv\"\n",
    "OUT_UNMATCHED = TIDY_DIR / \"all_properties_tidy_UNMATCHED_SUBURBS.csv\"\n",
    "\n",
    "# Create directories\n",
    "for p in [SPLIT_DIR, STRIPPED_DIR, TIDY_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print paths\n",
    "print(\"Paths:\")\n",
    "print(\"  EXCEL_PATH   =\", EXCEL_PATH)\n",
    "print(\"  MAP_CSV      =\", MAP_CSV)\n",
    "print(\"  SPLIT_DIR    =\", SPLIT_DIR)\n",
    "print(\"  STRIPPED_DIR =\", STRIPPED_DIR)\n",
    "print(\"  TIDY_DIR     =\", TIDY_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e69d134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found sheets: ['1 bedroom flat', '2 bedroom flat', '3 bedroom flat', '2 bedroom house', '3 bedroom house', '4 bedroom house', 'All properties']\n",
      "[split] Saved: 1_bedroom_flat.csv (161 rows)\n",
      "[split] Saved: 2_bedroom_flat.csv (161 rows)\n",
      "[split] Saved: 3_bedroom_flat.csv (161 rows)\n",
      "[split] Saved: 2_bedroom_house.csv (161 rows)\n",
      "[split] Saved: 3_bedroom_house.csv (161 rows)\n",
      "[split] Saved: 4_bedroom_house.csv (161 rows)\n",
      "[split] Saved: all_properties.csv (161 rows)\n"
     ]
    }
   ],
   "source": [
    "def _safe_name(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", s).strip(\"_\").lower()\n",
    "\n",
    "if not EXCEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Excel not found: {EXCEL_PATH}\\nCheck your PROJECT_ROOT or filename.\")\n",
    "\n",
    " # needs openpyxl\n",
    "xls = pd.ExcelFile(EXCEL_PATH) \n",
    "print(\"Found sheets:\", xls.sheet_names)\n",
    "\n",
    "for sheet in xls.sheet_names:\n",
    "    df = pd.read_excel(EXCEL_PATH, sheet_name=sheet)\n",
    "    out_csv = SPLIT_DIR / f\"{_safe_name(sheet)}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[split] Saved: {out_csv.name} ({len(df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a96cba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[strip] 1_bedroom_flat.csv → 147 rows\n",
      "[strip] 2_bedroom_flat.csv → 147 rows\n",
      "[strip] 2_bedroom_house.csv → 147 rows\n",
      "[strip] 3_bedroom_flat.csv → 147 rows\n",
      "[strip] 3_bedroom_house.csv → 147 rows\n",
      "[strip] 4_bedroom_house.csv → 147 rows\n",
      "[strip] all_properties.csv → 147 rows\n"
     ]
    }
   ],
   "source": [
    "def strip_sheet(in_csv: Path, out_csv: Path) -> int:\n",
    "    # Read raw with no header so we can promote rows later if needed\n",
    "    df = pd.read_csv(in_csv, header=None, dtype=str)\n",
    "\n",
    "    # Need at least 2 rows/cols to drop first row/col safely\n",
    "    if df.shape[0] >= 2 and df.shape[1] >= 2:\n",
    "        # Drop first row & first col (typical for these spreadsheets)\n",
    "        df2 = df.iloc[1:, 1:].reset_index(drop=True)\n",
    "        # Promote next row to header\n",
    "        new_header = df2.iloc[0].astype(str).str.strip()\n",
    "        df2 = df2.iloc[1:].reset_index(drop=True)\n",
    "        df2.columns = new_header\n",
    "    else:\n",
    "        df2 = pd.read_csv(in_csv, dtype=str)\n",
    "\n",
    "    # Drop unnamed columns, empty rows\n",
    "    df2 = df2.loc[:, ~df2.columns.str.contains(\"^Unnamed\", case=False)]\n",
    "    df2 = df2.dropna(how=\"all\")\n",
    "\n",
    "    # Force first column header to 'Suburb'\n",
    "    if df2.columns.size > 0:\n",
    "        df2.columns = [\"Suburb\"] + df2.columns[1:].tolist()\n",
    "\n",
    "    # Drop summary rows like \"Group Total\"\n",
    "    mask_total = df2.apply(lambda r: r.astype(str).str.contains(r\"\\bgroup\\s*total\\b\", case=False, regex=True).any(), axis=1)\n",
    "    df2 = df2.loc[~mask_total]\n",
    "\n",
    "    df2.to_csv(out_csv, index=False)\n",
    "    return len(df2)\n",
    "\n",
    "produced = []\n",
    "for p in sorted(SPLIT_DIR.glob(\"*.csv\")):\n",
    "    out = STRIPPED_DIR / p.name\n",
    "    n = strip_sheet(p, out)\n",
    "    produced.append(out)\n",
    "    print(f\"[strip] {p.name} → {n} rows\")\n",
    "\n",
    "if not produced:\n",
    "    print(\"No stripped files produced — check the split step.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c771800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(_clean_cell)\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(_clean_cell)\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(_clean_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tidy] 1_bedroom_flat.csv         → 13538 rows\n",
      "[tidy] 2_bedroom_flat.csv         → 14607 rows\n",
      "[tidy] 2_bedroom_house.csv        → 14124 rows\n",
      "[tidy] 3_bedroom_flat.csv         → 13978 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(_clean_cell)\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(_clean_cell)\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(_clean_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tidy] 3_bedroom_house.csv        → 14428 rows\n",
      "[tidy] 4_bedroom_house.csv        → 14054 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
      "/var/folders/59/n7t8nd2s08zcjsn4bd22nlsr0000gn/T/ipykernel_57050/3719917520.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(_clean_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tidy] all_properties.csv         → 14739 rows\n",
      "\n",
      "[done] Combined → ../../datasets/raw/tidy_monthly/all_properties_tidy.csv  rows=99,468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MONTH_ABBR = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "FILE_RX = re.compile(r\"(?P<beds>\\d+)_bedroom_(?P<ptype>flat|house)\", re.I)\n",
    "\n",
    "def _clean_cell(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return None\n",
    "    x = str(x).strip()\n",
    "    return None if x in {\"\", \"-\"} else x\n",
    "\n",
    "def load_and_build_header(path: Path) -> pd.DataFrame:\n",
    "    raw = pd.read_csv(path, header=None, dtype=str)\n",
    "    raw = raw.applymap(lambda x: None if pd.isna(x) else str(x).strip())\n",
    "\n",
    "    r0 = raw.iloc[0].tolist() if len(raw) > 0 else []\n",
    "    r1 = raw.iloc[1].tolist() if len(raw) > 1 else []\n",
    "\n",
    "    def looks_like_two_row_header(r0, r1):\n",
    "        has_month = any(any(m in str(c) for m in MONTH_ABBR) for c in r0)\n",
    "        has_metrics = any(str(c).strip().lower() in {\"count\",\"median\"} for c in r1)\n",
    "        return has_month and has_metrics\n",
    "\n",
    "    if looks_like_two_row_header(r0, r1):\n",
    "        # Forward-fill top row months\n",
    "        r0_ffill, last = [], None\n",
    "        for c in r0:\n",
    "            if c not in [None,\"\",\"nan\"]:\n",
    "                last = c\n",
    "            r0_ffill.append(last)\n",
    "        cols = []\n",
    "        for i, (top, sub) in enumerate(zip(r0_ffill, r1)):\n",
    "            top = \"Suburb\" if i == 0 else top\n",
    "            name = top if i == 0 else f\"{top}_{sub}\"\n",
    "            cols.append(name)\n",
    "        df = raw.iloc[2:].copy()\n",
    "        df.columns = cols\n",
    "    else:\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "        if not df.columns[0] or str(df.columns[0]).lower().startswith(\"unnamed\"):\n",
    "            df.rename(columns={df.columns[0]: \"Suburb\"}, inplace=True)\n",
    "\n",
    "    df = df.applymap(_clean_cell)\n",
    "    if \"Suburb\" not in df.columns:\n",
    "        df.rename(columns={df.columns[0]: \"Suburb\"}, inplace=True)\n",
    "    return df.dropna(how=\"all\")\n",
    "\n",
    "def wide_to_tidy(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    val_cols = [c for c in df.columns if c != \"Suburb\"]\n",
    "    long = df.melt(id_vars=[\"Suburb\"], value_vars=val_cols,\n",
    "                   var_name=\"period_metric\", value_name=\"value\")\n",
    "    ext = long[\"period_metric\"].str.extract(r\"(?P<period>[A-Za-z]{3}\\s+\\d{4})[_\\s-]*(?P<metric>Count|Median)?\")\n",
    "    long[\"period\"] = ext[\"period\"]\n",
    "    long[\"metric\"] = ext[\"metric\"].fillna(\"\")\n",
    "    tidy = (long.pivot_table(index=[\"Suburb\",\"period\"], columns=\"metric\", values=\"value\", aggfunc=\"first\")\n",
    "                 .reset_index())\n",
    "    tidy.columns.name = None\n",
    "    if \"Count\" in tidy.columns:\n",
    "        tidy[\"Count\"] = pd.to_numeric(tidy[\"Count\"], errors=\"coerce\")\n",
    "    if \"Median\" in tidy.columns:\n",
    "        tidy[\"Median\"] = pd.to_numeric(tidy[\"Median\"], errors=\"coerce\")\n",
    "    tidy[\"Date\"] = pd.to_datetime(tidy[\"period\"], format=\"%b %Y\", errors=\"coerce\")\n",
    "    tidy = tidy.drop(columns=[\"period\"])\n",
    "    cols = [\"Suburb\",\"Date\"] + [c for c in [\"Count\",\"Median\"] if c in tidy.columns]\n",
    "    return tidy[cols].sort_values([\"Suburb\",\"Date\"]).reset_index(drop=True)\n",
    "\n",
    "def annotate_from_filename(path: Path) -> dict:\n",
    "    m = FILE_RX.search(path.stem)\n",
    "    if not m:\n",
    "        return {\"Bedrooms\": None, \"Dwelling\": None}\n",
    "    return {\"Bedrooms\": int(m.group(\"beds\")), \"Dwelling\": (\"apartment\" if m.group(\"ptype\").lower()==\"flat\" else m.group(\"ptype\").lower())}\n",
    "\n",
    "all_frames = []\n",
    "for p in sorted(STRIPPED_DIR.glob(\"*.csv\")):\n",
    "    try:\n",
    "        df = load_and_build_header(p)\n",
    "        tidy = wide_to_tidy(df)\n",
    "        meta = annotate_from_filename(p)\n",
    "        for k,v in meta.items():\n",
    "            tidy[k] = v\n",
    "        # reorder\n",
    "        base = [\"Suburb\",\"Date\",\"Bedrooms\",\"Dwelling\"]\n",
    "        vals = [c for c in [\"Count\",\"Median\"] if c in tidy.columns]\n",
    "        tidy = tidy[base + vals]\n",
    "        all_frames.append(tidy)\n",
    "        # also per-file tidy for inspection\n",
    "        (TIDY_DIR / f\"{p.stem}_tidy.csv\").write_text(tidy.to_csv(index=False))\n",
    "        print(f\"[tidy] {p.name:26s} → {len(tidy):5d} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"[skip] {p.name}: {e}\")\n",
    "\n",
    "if all_frames:\n",
    "    combo = (pd.concat(all_frames, ignore_index=True)\n",
    "               .sort_values([\"Suburb\",\"Date\",\"Bedrooms\",\"Dwelling\"]))\n",
    "    combo.to_csv(OUT_TIDY, index=False)\n",
    "    print(f\"\\n[done] Combined → {OUT_TIDY}  rows={len(combo):,}\")\n",
    "else:\n",
    "    print(\"No tidy frames produced — check stripped files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d37aceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../../datasets/raw/tidy_monthly/all_properties_tidy_enriched.csv — matched 98774/99468 (99.3%)\n",
      "Unmatched list saved: ../../datasets/raw/tidy_monthly/all_properties_tidy_UNMATCHED_SUBURBS.csv\n",
      "Top few:\n",
      "         Suburb  count\n",
      "0  Wanagaratta    694\n"
     ]
    }
   ],
   "source": [
    "# 1) Paths\n",
    "PROJECT_ROOT = Path(\"../../\") \n",
    "DATA_ROOT    = PROJECT_ROOT / \"datasets\"\n",
    "TIDY_CSV     = DATA_ROOT / \"raw/tidy_monthly/all_properties_tidy.csv\"\n",
    "OUT_ENRICHED = DATA_ROOT / \"raw/tidy_monthly/all_properties_tidy_enriched.csv\"\n",
    "OUT_UNMATCHED= DATA_ROOT / \"raw/tidy_monthly/all_properties_tidy_UNMATCHED_SUBURBS.csv\"\n",
    "\n",
    "MAP_CSV = DATA_ROOT / \"district_shape/sa2_lookup/mapped_target_suburbs.csv\"\n",
    "\n",
    "if not MAP_CSV.exists():\n",
    "    print(f\"Mapping not found at:\\n  {MAP_CSV}\\nTrying to auto-locate it under {PROJECT_ROOT} ...\")\n",
    "    found = list(PROJECT_ROOT.rglob(\"mapped_target_suburbs.csv\"))\n",
    "    if found:\n",
    "        MAP_CSV = found[0]\n",
    "        print(f\"Found mapping at:\\n  {MAP_CSV}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Mapping CSV not found. Please check the path above.\")\n",
    "\n",
    "# 2) Helpers\n",
    "def norm(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.normalize(\"NFKD\").str.encode(\"ascii\",\"ignore\").str.decode(\"ascii\")\n",
    "         .str.upper().str.strip()\n",
    "         .str.replace(r\"[^\\w\\s\\-]\", \" \", regex=True)\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    )\n",
    "\n",
    "# 3) Load data\n",
    "if not TIDY_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Tidy file not found: {TIDY_CSV}\\nRun the tidy build step first.\")\n",
    "df     = pd.read_csv(TIDY_CSV)\n",
    "map_df = pd.read_csv(MAP_CSV)\n",
    "\n",
    "# 4) Robust column detection in mapping file\n",
    "up_map = {c: c.upper() for c in map_df.columns}\n",
    "def need(name):\n",
    "    for c,u in up_map.items():\n",
    "        if u == name:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "map_suburb_col = need(\"TARGET_SUBURB\") or need(\"SUBURB\") or need(\"SUBURB_NAME\") or need(\"LOCALITY\")\n",
    "lat_col = need(\"LAT\"); lng_col = need(\"LNG\")\n",
    "sa2c = need(\"SA2_CODE21\"); sa2n = need(\"SA2_NAME21\")\n",
    "lgac = need(\"LGA_CODE21\"); lgan = need(\"LGA_NAME21\")\n",
    "\n",
    "if any(x is None for x in [map_suburb_col, lat_col, lng_col, sa2c, sa2n, lgac, lgan]):\n",
    "    raise ValueError(f\"Mapping CSV missing required columns. Found: {map_df.columns.tolist()}\")\n",
    "\n",
    "# 5) Join on normalized suburb strings\n",
    "df[\"_SUBURB_KEY\"]     = norm(df[\"Suburb\"])\n",
    "map_df[\"_SUBURB_KEY\"] = norm(map_df[map_suburb_col])\n",
    "\n",
    "map_keep = map_df[[\"_SUBURB_KEY\", lat_col, lng_col, sa2c, sa2n, lgac, lgan]].rename(\n",
    "    columns={\n",
    "        lat_col:\"Lat\", lng_col:\"Lng\",\n",
    "        sa2c:\"SA2_CODE21\", sa2n:\"SA2_NAME21\",\n",
    "        lgac:\"LGA_CODE21\", lgan:\"LGA_NAME21\"\n",
    "    }\n",
    ")\n",
    "\n",
    "enriched = df.merge(map_keep, on=\"_SUBURB_KEY\", how=\"left\").drop(columns=[\"_SUBURB_KEY\"])\n",
    "\n",
    "# 6) Save + diagnostics\n",
    "enriched.to_csv(OUT_ENRICHED, index=False)\n",
    "matched = enriched[\"SA2_CODE21\"].notna().sum()\n",
    "print(f\"Saved: {OUT_ENRICHED} — matched {matched}/{len(enriched)} ({matched/len(enriched):.1%})\")\n",
    "\n",
    "# Unmatched suburb report to fix mapping gaps quickly\n",
    "unmatched = (\n",
    "    enriched.loc[enriched[\"SA2_CODE21\"].isna(), \"Suburb\"]\n",
    "    .dropna().astype(str).value_counts().reset_index()\n",
    ")\n",
    "if not unmatched.empty:\n",
    "    unmatched.columns = [\"Suburb\",\"count\"]\n",
    "    unmatched.to_csv(OUT_UNMATCHED, index=False)\n",
    "    print(f\"Unmatched list saved: {OUT_UNMATCHED}\\nTop few:\\n\", unmatched.head())\n",
    "else:\n",
    "    print(\"No unmatched suburbs \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
