{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35835d92",
   "metadata": {},
   "source": [
    "## Crime dataset ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc3f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Setup ---------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4779f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Input files (your absolute paths) ----\n",
    "P_CORR  = Path(\"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/CG_SA2_2021_LGA_2021.csv\")\n",
    "P_LGA   = Path(\"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb8b7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Output files ----\n",
    "OUT_DIR = P_LGA.parent\n",
    "OUT_SA2 = OUT_DIR / \"crime_dataset_weighted_to_SA2.csv\"\n",
    "OUT_CHECKS = OUT_DIR / \"weight_checks_by_LGA.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67f654a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1) Read data -----------------------------------------------------------\n",
    "# Read as strings to avoid code mangling (e.g., 2.01E+08)\n",
    "corr = pd.read_csv(P_CORR, dtype=str)\n",
    "lga  = pd.read_csv(P_LGA,  dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6f1eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2) Normalise/standardise column names ---------------------------------\n",
    "# Make a case-insensitive mapping to canonical names\n",
    "def normcols(df):\n",
    "    m = {c: c.strip() for c in df.columns}\n",
    "    df = df.rename(columns=m)\n",
    "    # Lowercase for matching only; keep originals for output later if you want\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "corr = normcols(corr)\n",
    "lga  = normcols(lga)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e036821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to detect expected key/weight columns in corr\n",
    "# Common possibilities: SA2_CODE_2021 / SA2_NAME_2021 / LGA_CODE_2021 / LGA_NAME_2021 / RATIO_FROM_TO or RATIO_TO\n",
    "def pick(name_options, cols):\n",
    "    for n in name_options:\n",
    "        if n in cols:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "c_sa2_code = pick([\"SA2_CODE_2021\",\"SA2_CODE21\",\"SA2_MAINCODE_2021\",\"SA2_MAINCODE21\",\"SA2_CODE\"], corr.columns)\n",
    "c_sa2_name = pick([\"SA2_NAME_2021\",\"SA2_NAME21\",\"SA2_NAME\"], corr.columns)\n",
    "c_lga_code = pick([\"LGA_CODE_2021\",\"LGA_CODE21\",\"LGA_CODE\"], corr.columns)\n",
    "c_lga_name = pick([\"LGA_NAME_2021\",\"LGA_NAME21\",\"LGA_NAME\"], corr.columns)\n",
    "c_ratio    = pick([\"RATIO_FROM_TO\",\"RATIO_TO\",\"RATIO\",\"PROP\",\"WEIGHT\"], corr.columns)\n",
    "\n",
    "missing = [x for x in [c_sa2_code,c_sa2_name,c_lga_code,c_lga_name,c_ratio] if x is None]\n",
    "if missing:\n",
    "    raise ValueError(f\"Could not find these columns in correspondence file: {missing}\\n\"\n",
    "                     f\"Have columns: {list(corr.columns)}\")\n",
    "\n",
    "# Ensure codes are strings and trimmed\n",
    "for c in [c_sa2_code, c_lga_code]:\n",
    "    corr[c] = corr[c].astype(str).str.strip()\n",
    "\n",
    "# Filter to Victoria SA2s only (ASGS 2021 VIC codes begin with '2')\n",
    "corr_vic = corr[corr[c_sa2_code].str.startswith(\"2\")].copy()\n",
    "\n",
    "# Prepare LGA dataset: ensure it has an LGA key\n",
    "l_lga_code = pick([\"LGA_CODE_2021\",\"LGA_CODE21\",\"LGA_CODE\"], lga.columns)\n",
    "l_lga_name = pick([\"LGA_NAME_2021\",\"LGA_NAME21\",\"LGA_NAME\"], lga.columns)\n",
    "\n",
    "if (l_lga_code is None) and (l_lga_name is None):\n",
    "    raise ValueError(f\"LGA dataset has no LGA code/name column. Found columns: {list(lga.columns)}\")\n",
    "\n",
    "# Coerce numeric measure columns later; for now keep everything as str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ed9cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3) Build per-LGA normalised weights -----------------------------------\n",
    "# We’ll normalise weights WITHIN each LGA so that allocations from that LGA\n",
    "# to its SA2 parts sum to 1. This guarantees LGA totals are preserved.\n",
    "\n",
    "# Use the ratio column (population-based overlap) then normalise by LGA\n",
    "corr_vic[\"ratio_raw\"] = pd.to_numeric(corr_vic[c_ratio], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# Key to group by for normalisation\n",
    "grp_key = c_lga_code if l_lga_code else c_lga_name\n",
    "\n",
    "# If we’re grouping by name, lower-case/strip both sides for robust matches\n",
    "if grp_key == c_lga_name:\n",
    "    corr_vic[c_lga_name + \"_key\"] = corr_vic[c_lga_name].str.strip().str.lower()\n",
    "    lga[l_lga_name + \"_key\"]       = lga[l_lga_name].str.strip().str.lower()\n",
    "    corr_group_key = c_lga_name + \"_key\"\n",
    "    lga_join_key   = l_lga_name + \"_key\"\n",
    "else:\n",
    "    # code path\n",
    "    corr_vic[c_lga_code + \"_key\"] = corr_vic[c_lga_code].str.strip()\n",
    "    lga[l_lga_code + \"_key\"]       = lga[l_lga_code].str.strip()\n",
    "    corr_group_key = c_lga_code + \"_key\"\n",
    "    lga_join_key   = l_lga_code + \"_key\"\n",
    "\n",
    "# Normalise by LGA so weights sum to 1 within each LGA\n",
    "sum_by_lga = corr_vic.groupby(corr_vic[corr_group_key])[\"ratio_raw\"].sum().rename(\"ratio_sum_lga\")\n",
    "corr_vic = corr_vic.merge(sum_by_lga, left_on=corr_group_key, right_index=True, how=\"left\")\n",
    "# Avoid divide-by-zero; if an LGA has zero sum (shouldn’t), fall back to equal split\n",
    "corr_vic[\"weight_norm\"] = np.where(\n",
    "    corr_vic[\"ratio_sum_lga\"] > 0,\n",
    "    corr_vic[\"ratio_raw\"] / corr_vic[\"ratio_sum_lga\"],\n",
    "    1.0 / corr_vic.groupby(corr_vic[corr_group_key])[corr_group_key].transform(\"count\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec4d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4) Join LGA data to weights -------------------------------------------\n",
    "# Identify numeric measure columns in LGA data:\n",
    "# Counts -> sum after weighting; Rates -> weighted mean after weighting\n",
    "def starts_with_any(s, prefixes):\n",
    "    return any(s.startswith(p) for p in prefixes)\n",
    "\n",
    "count_prefixes = (\"Incidents_\", \"Victims_\")\n",
    "rate_prefixes  = (\"CrimeRate_\", \"VictimRate_\")\n",
    "\n",
    "# infer numeric columns by try-convert\n",
    "lga_numeric = []\n",
    "for c in lga.columns:\n",
    "    # skip keys\n",
    "    if c in [l_lga_code, l_lga_name, lga_join_key]:\n",
    "        continue\n",
    "    # try numeric conversion\n",
    "    try:\n",
    "        pd.to_numeric(lga[c])\n",
    "        lga_numeric.append(c)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "count_cols = [c for c in lga_numeric if starts_with_any(c, count_prefixes)]\n",
    "rate_cols  = [c for c in lga_numeric if starts_with_any(c, rate_prefixes)]\n",
    "\n",
    "# Merge LGA data to correspondence weights\n",
    "to_merge_cols = [col for col in [l_lga_code, l_lga_name, lga_join_key] if col is not None]\n",
    "lga_for_merge = lga[to_merge_cols + count_cols + rate_cols].copy()\n",
    "\n",
    "merged = corr_vic.merge(\n",
    "    lga_for_merge,\n",
    "    left_on=corr_group_key,\n",
    "    right_on=lga_join_key,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Coerce measures to numeric (errors -> NaN -> 0)\n",
    "for c in count_cols + rate_cols:\n",
    "    merged[c] = pd.to_numeric(merged[c], errors=\"coerce\").fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc65dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5) Apply weights -------------------------------------------------------\n",
    "# For counts: allocate by weight_norm and then sum to SA2\n",
    "for c in count_cols:\n",
    "    merged[c + \"_alloc\"] = merged[c] * merged[\"weight_norm\"]\n",
    "\n",
    "# For rates: weighted mean using weight_norm\n",
    "for c in rate_cols:\n",
    "    merged[c + \"_alloc\"] = merged[c] * merged[\"weight_norm\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c999a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6) Aggregate to SA2 (unique) ------------------------------------------\n",
    "sa2_keys = [c_sa2_code, c_sa2_name]\n",
    "alloc_count_cols = [c + \"_alloc\" for c in count_cols]\n",
    "alloc_rate_cols  = [c + \"_alloc\" for c in rate_cols]\n",
    "\n",
    "sa2_counts = (\n",
    "    merged.groupby(sa2_keys, dropna=False)[alloc_count_cols]\n",
    "          .sum()\n",
    "          .rename(columns=lambda x: x.replace(\"_alloc\",\"\"))\n",
    ")\n",
    "\n",
    "# For rates, we summed rate * weight; need to divide by sum of weights per SA2 (which should be 1 if every LGA covering\n",
    "# the SA2 is present; to be safe, recompute per-SA2 total weight from the same merge).\n",
    "sa2_weight_sum = merged.groupby(sa2_keys, dropna=False)[\"weight_norm\"].sum().rename(\"weight_sum_sa2\")\n",
    "\n",
    "sa2_rates_sum = (\n",
    "    merged.groupby(sa2_keys, dropna=False)[alloc_rate_cols]\n",
    "          .sum()\n",
    "          .rename(columns=lambda x: x.replace(\"_alloc\",\"_weighted_sum\"))\n",
    ")\n",
    "sa2_rates = sa2_rates_sum.copy()\n",
    "for c in rate_cols:\n",
    "    # divide by sum of weights; avoid zero-div\n",
    "    sa2_rates[c] = np.where(sa2_weight_sum.values > 0,\n",
    "                            sa2_rates_sum[c + \"_weighted_sum\"].values / sa2_weight_sum.values,\n",
    "                            np.nan)\n",
    "    del sa2_rates[c + \"_weighted_sum\"]\n",
    "\n",
    "# Combine\n",
    "sa2_final = sa2_counts.join(sa2_rates, how=\"outer\").reset_index()\n",
    "\n",
    "# Optional: keep lat/lng if present in your LGA file (usually not). Skip unless you have a rule to pick one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea172708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. SA2-level dataset written to:\n",
      "  /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_dataset_weighted_to_SA2.csv\n",
      "Allocation check by LGA written to:\n",
      "  /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/weight_checks_by_LGA.csv\n",
      "(abs_diff should be ~0 per LGA per measure)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 7) Sanity checks & exports --------------------------------------------\n",
    "# Check that for each LGA, sum of allocated counts across its SA2s ≈ original LGA totals\n",
    "# (floating point tolerance)\n",
    "checks = []\n",
    "for c in count_cols:\n",
    "    lga_alloc_back = (\n",
    "        merged.groupby(lga_join_key)[c + \"_alloc\"].sum().rename(\"allocated_sum\")\n",
    "        .reset_index()\n",
    "        .merge(lga_for_merge[[lga_join_key, c]], on=lga_join_key, how=\"left\")\n",
    "    )\n",
    "    lga_alloc_back[\"abs_diff\"] = (lga_alloc_back[\"allocated_sum\"] - pd.to_numeric(lga_alloc_back[c], errors=\"coerce\")).abs()\n",
    "    lga_alloc_back[\"measure\"]  = c\n",
    "    checks.append(lga_alloc_back[[lga_join_key,\"measure\",\"allocated_sum\",c,\"abs_diff\"]])\n",
    "\n",
    "weight_checks = pd.concat(checks, ignore_index=True) if checks else pd.DataFrame()\n",
    "weight_checks.to_csv(OUT_CHECKS, index=False)\n",
    "\n",
    "# Export the SA2 dataset\n",
    "sa2_final.to_csv(OUT_SA2, index=False)\n",
    "\n",
    "print(f\"Done. SA2-level dataset written to:\\n  {OUT_SA2}\")\n",
    "if not weight_checks.empty:\n",
    "    print(f\"Allocation check by LGA written to:\\n  {OUT_CHECKS}\\n\"\n",
    "          f\"(abs_diff should be ~0 per LGA per measure)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9414a96",
   "metadata": {},
   "source": [
    "## CRIME PREDICTION TILL 2030 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9ae23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_dataset_weighted_to_SA2.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f8533ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --------------------------------- CONFIG & IMPORTS ---------------------------------\n",
    "import warnings, os, re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import pmdarima; if not installed, fallback will still work\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    HAS_ARIMA = True\n",
    "except Exception:\n",
    "    HAS_ARIMA = False\n",
    "\n",
    "# ---- Input file (already weighted to SA2) ----\n",
    "IN_PATH = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_dataset_weighted_to_SA2.csv\"\n",
    "\n",
    "# ---- Output directory (same parent as IN_PATH by default) ----\n",
    "OUT_DIR = os.path.dirname(IN_PATH)  # you can change if you want outputs elsewhere\n",
    "YEARLY_OUT = os.path.join(OUT_DIR, \"crime_predictions_yearly_to_2030.csv\")\n",
    "QUARTERLY_OUT = os.path.join(OUT_DIR, \"crime_predictions_quarterly_to_2030.csv\")\n",
    "\n",
    "END_YEAR = 2030  # inclusive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64c5616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long shape: (15660, 5)\n",
      "Metrics: ['CrimeRate' 'Incidents' 'VictimRate']\n",
      "Year range: 2016 → 2025\n"
     ]
    }
   ],
   "source": [
    "# %% ------------------------------- LOAD & NORMALISE (WIDE -> LONG) -------------------\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(IN_PATH)\n",
    "\n",
    "# keep identifiers\n",
    "id_cols = [c for c in (\"SA2_CODE_2021\", \"SA2_NAME_2021\") if c in df.columns]\n",
    "if \"SA2_CODE_2021\" not in id_cols:\n",
    "    raise ValueError(\"Expected SA2_CODE_2021 in the file. Found: {}\".format(df.columns.tolist()))\n",
    "\n",
    "# we ONLY want these 3 metric families\n",
    "want_metrics = (\"Incidents\", \"CrimeRate\", \"VictimRate\")\n",
    "pat = re.compile(rf\"^({'|'.join(want_metrics)})_(\\d{{4}})$\")\n",
    "\n",
    "# collect (col_name, metric, year) triples that match\n",
    "metric_year_cols = []\n",
    "for c in df.columns:\n",
    "    m = pat.match(c)\n",
    "    if m:\n",
    "        metric_year_cols.append((c, m.group(1), int(m.group(2))))\n",
    "\n",
    "if not metric_year_cols:\n",
    "    raise ValueError(\"No (Incidents|CrimeRate|VictimRate)_YYYY columns found. \"\n",
    "                     f\"Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "# build long frame by stacking the matching columns\n",
    "long_parts = []\n",
    "for col, metric, year in metric_year_cols:\n",
    "    part = df[id_cols + [col]].copy()\n",
    "    part[\"metric\"] = metric\n",
    "    part[\"Year\"] = year\n",
    "    part = part.rename(columns={col: \"value\"})\n",
    "    long_parts.append(part)\n",
    "\n",
    "long = (\n",
    "    pd.concat(long_parts, ignore_index=True)\n",
    "      .assign(value=lambda x: pd.to_numeric(x[\"value\"], errors=\"coerce\"))\n",
    "      .dropna(subset=[\"value\"])\n",
    "      .sort_values([\"SA2_CODE_2021\", \"metric\", \"Year\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Long shape:\", long.shape)\n",
    "print(\"Metrics:\", long[\"metric\"].unique())\n",
    "print(\"Year range:\", int(long[\"Year\"].min()), \"→\", int(long[\"Year\"].max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64201ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Long shape: (15660, 5) | Metrics: ['CrimeRate', 'Incidents', 'VictimRate'] | Years: 2016–2025\n",
      "[OK] Yearly forecasts saved -> /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_predictions_yearly_to_2030.csv\n",
      "[OK] Quarterly panel saved -> /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_predictions_quarterly_to_2030.csv\n",
      "\n",
      "SAMPLES (Yearly):\n",
      " SA2_CODE_2021 SA2_NAME_2021  Year  Incidents   CrimeRate  VictimRate\n",
      "     201011001     Alfredton  2016    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2017    14907.0 8730.376785 5300.755085\n",
      "     201011001     Alfredton  2018    14394.0 8268.901940 4992.193889\n",
      "     201011001     Alfredton  2019    13148.0 7404.370838 4344.419848\n",
      "     201011001     Alfredton  2020    14095.0 7804.200815 4677.325451\n",
      "     201011001     Alfredton  2021    11626.0 6331.935438 3521.461799\n",
      "     201011001     Alfredton  2022    12241.0 6522.657334 3882.390463\n",
      "     201011001     Alfredton  2023    13308.0 6964.597125 3769.703537\n",
      "\n",
      "SAMPLES (Quarterly):\n",
      " SA2_CODE_2021 SA2_NAME_2021  Year Quarter                    QuarterEnd  Incidents   CrimeRate  VictimRate\n",
      "     201011001     Alfredton  2016      Q1 2016-03-31 23:59:59.999999999    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2016      Q2 2016-06-30 23:59:59.999999999    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2016      Q3 2016-09-30 23:59:59.999999999    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2016      Q4 2016-12-31 23:59:59.999999999    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2017      Q1 2017-03-31 23:59:59.999999999    14680.0 8725.324856 5631.765829\n",
      "     201011001     Alfredton  2017      Q2 2017-06-30 23:59:59.999999999    14755.0 8726.996585 5522.231365\n",
      "     201011001     Alfredton  2017      Q3 2017-09-30 23:59:59.999999999    14831.0 8728.686685 5411.493225\n",
      "     201011001     Alfredton  2017      Q4 2017-12-31 23:59:59.999999999    14907.0 8730.376785 5300.755085\n",
      "     201011001     Alfredton  2018      Q1 2018-03-31 23:59:59.999999999    14780.0 8616.588467 5224.671503\n",
      "     201011001     Alfredton  2018      Q2 2018-06-30 23:59:59.999999999    14652.0 8501.535834 5147.742547\n",
      "     201011001     Alfredton  2018      Q3 2018-09-30 23:59:59.999999999    14523.0 8385.218887 5069.968218\n",
      "     201011001     Alfredton  2018      Q4 2018-12-31 23:59:59.999999999    14394.0 8268.901940 4992.193889\n"
     ]
    }
   ],
   "source": [
    "# %% --------------------------------- CONFIG & IMPORTS ---------------------------------\n",
    "import warnings, os, re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import pmdarima; if not installed or fails, we fall back to linear trend\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    HAS_ARIMA = True\n",
    "except Exception:\n",
    "    HAS_ARIMA = False\n",
    "\n",
    "# ---- Input file (already weighted to SA2; wide format with *_YYYY columns) ----\n",
    "IN_PATH = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_dataset_weighted_to_SA2.csv\"\n",
    "\n",
    "# ---- Outputs ----\n",
    "OUT_DIR = os.path.dirname(IN_PATH)\n",
    "YEARLY_OUT = os.path.join(OUT_DIR, \"crime_predictions_yearly_to_2030.csv\")\n",
    "QUARTERLY_OUT = os.path.join(OUT_DIR, \"crime_predictions_quarterly_to_2030.csv\")\n",
    "\n",
    "END_YEAR = 2030  # inclusive\n",
    "\n",
    "# %% ------------------------------- LOAD & NORMALISE (WIDE -> LONG) -------------------\n",
    "df = pd.read_csv(IN_PATH)\n",
    "\n",
    "# Ensure SA2 identifier exists\n",
    "id_cols = [c for c in (\"SA2_CODE_2021\",\"SA2_NAME_2021\") if c in df.columns]\n",
    "if \"SA2_CODE_2021\" not in id_cols:\n",
    "    raise ValueError(f\"Expected SA2_CODE_2021 column. Found: {df.columns.tolist()}\")\n",
    "\n",
    "# We only want these three metric families\n",
    "want_metrics = (\"Incidents\", \"CrimeRate\", \"VictimRate\")\n",
    "pat = re.compile(rf\"^({'|'.join(want_metrics)})_(\\d{{4}})$\")\n",
    "\n",
    "metric_year_cols = []\n",
    "for c in df.columns:\n",
    "    m = pat.match(c)\n",
    "    if m:\n",
    "        metric_year_cols.append((c, m.group(1), int(m.group(2))))\n",
    "\n",
    "if not metric_year_cols:\n",
    "    raise ValueError(\"No (Incidents|CrimeRate|VictimRate)_YYYY columns found.\\n\"\n",
    "                     f\"Available: {df.columns.tolist()}\")\n",
    "\n",
    "# Stack into long format: SA2_CODE_2021, optional SA2_NAME_2021, metric, Year, value\n",
    "long_parts = []\n",
    "for col, metric, year in metric_year_cols:\n",
    "    part = df[id_cols + [col]].copy()\n",
    "    part[\"metric\"] = metric\n",
    "    part[\"Year\"] = year\n",
    "    part = part.rename(columns={col: \"value\"})\n",
    "    long_parts.append(part)\n",
    "\n",
    "long = (\n",
    "    pd.concat(long_parts, ignore_index=True)\n",
    "      .assign(value=lambda x: pd.to_numeric(x[\"value\"], errors=\"coerce\"))\n",
    "      .dropna(subset=[\"value\"])\n",
    "      .sort_values([\"SA2_CODE_2021\",\"metric\",\"Year\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Long shape: {long.shape} | Metrics: {sorted(long['metric'].unique())} | \"\n",
    "      f\"Years: {int(long['Year'].min())}–{int(long['Year'].max())}\")\n",
    "\n",
    "# %% ----------------------------- FORECASTING HELPERS ----------------------------------\n",
    "def linear_trend_forecast(years, values, target_years):\n",
    "    years = np.array(years, dtype=float)\n",
    "    values = np.array(values, dtype=float)\n",
    "    target_years = np.array(target_years, dtype=float)\n",
    "    if len(np.unique(values)) == 1:\n",
    "        return np.full(len(target_years), float(values[-1]))\n",
    "    coeffs = np.polyfit(years, values, deg=1)    # slope, intercept\n",
    "    return coeffs[0] * target_years + coeffs[1]\n",
    "\n",
    "def arima_or_trend_predict(years, values, future_years, prefer_arima=True):\n",
    "    \"\"\"Try ARIMA (if available & series has enough variation), else linear trend.\"\"\"\n",
    "    y = pd.Series(values, dtype=float)\n",
    "    can_arima = prefer_arima and HAS_ARIMA and len(y) >= 5 and not np.allclose(y, y.iloc[0])\n",
    "    if can_arima:\n",
    "        try:\n",
    "            model = auto_arima(\n",
    "                y, seasonal=False, stationary=False, information_criterion=\"bic\",\n",
    "                suppress_warnings=True, error_action=\"ignore\",\n",
    "                max_p=3, max_q=3, max_d=2, stepwise=True\n",
    "            )\n",
    "            return np.asarray(model.predict(n_periods=len(future_years)), dtype=float)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return linear_trend_forecast(years, values, future_years)\n",
    "\n",
    "# %% ----------------------------- BUILD YEARLY FORECASTS -------------------------------\n",
    "yearly_frames = []\n",
    "\n",
    "group_keys = [\"SA2_CODE_2021\",\"metric\"]\n",
    "if \"SA2_NAME_2021\" in long.columns:\n",
    "    group_keys = [\"SA2_CODE_2021\",\"SA2_NAME_2021\",\"metric\"]\n",
    "\n",
    "for keys, g in long.groupby(group_keys):\n",
    "    g = g.sort_values(\"Year\")\n",
    "    hist_years  = g[\"Year\"].tolist()\n",
    "    hist_values = g[\"value\"].tolist()\n",
    "\n",
    "    last_hist = max(hist_years)\n",
    "    fut_years = list(range(last_hist + 1, END_YEAR + 1))\n",
    "\n",
    "    out_years  = hist_years.copy()\n",
    "    out_values = hist_values.copy()\n",
    "\n",
    "    if fut_years:\n",
    "        preds = arima_or_trend_predict(hist_years, hist_values, fut_years, prefer_arima=True)\n",
    "        out_years.extend(fut_years)\n",
    "        out_values.extend(preds.tolist())\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        \"SA2_CODE_2021\": g[\"SA2_CODE_2021\"].iloc[0],\n",
    "        \"metric\": g[\"metric\"].iloc[0],\n",
    "        \"Year\": out_years,\n",
    "        \"value\": out_values\n",
    "    })\n",
    "    if \"SA2_NAME_2021\" in g.columns:\n",
    "        tmp[\"SA2_NAME_2021\"] = g[\"SA2_NAME_2021\"].iloc[0]\n",
    "\n",
    "    # Non-negative counts for Incidents\n",
    "    if tmp[\"metric\"].iloc[0] == \"Incidents\":\n",
    "        tmp[\"value\"] = tmp[\"value\"].clip(lower=0)\n",
    "\n",
    "    yearly_frames.append(tmp)\n",
    "\n",
    "yearly_all = pd.concat(yearly_frames, ignore_index=True)\n",
    "\n",
    "# Round Incidents in the SAVED yearly file; keep float internally if you’d like\n",
    "yearly_save = yearly_all.copy()\n",
    "mask_inc = yearly_save[\"metric\"] == \"Incidents\"\n",
    "yearly_save.loc[mask_inc, \"value\"] = np.round(yearly_save.loc[mask_inc, \"value\"]).astype(int)\n",
    "\n",
    "# Pivot to wide (Yearly) and save\n",
    "index_cols = [\"SA2_CODE_2021\",\"Year\"]\n",
    "if \"SA2_NAME_2021\" in yearly_save.columns:\n",
    "    index_cols = [\"SA2_CODE_2021\",\"SA2_NAME_2021\",\"Year\"]\n",
    "\n",
    "yearly_wide = (\n",
    "    yearly_save\n",
    "    .pivot_table(index=index_cols, columns=\"metric\", values=\"value\", aggfunc=\"first\")\n",
    "    .reset_index()\n",
    "    .sort_values([\"SA2_CODE_2021\",\"Year\"])\n",
    ")\n",
    "\n",
    "# Neat column order\n",
    "ordered_cols = [c for c in [\"SA2_CODE_2021\",\"SA2_NAME_2021\",\"Year\",\"Incidents\",\"CrimeRate\",\"VictimRate\"] if c in yearly_wide.columns]\n",
    "yearly_wide = yearly_wide[ordered_cols + [c for c in yearly_wide.columns if c not in ordered_cols]]\n",
    "\n",
    "yearly_wide.to_csv(YEARLY_OUT, index=False)\n",
    "print(f\"[OK] Yearly forecasts saved -> {YEARLY_OUT}\")\n",
    "\n",
    "# %% -------------------------- QUARTERLY INTERPOLATION (Q1–Q4) -------------------------\n",
    "# Strategy:\n",
    "# - Treat each yearly value as at Q4 (Dec-31) using 'A-DEC' freq.\n",
    "# - Build a full quarterly index (Q-DEC) and interpolate linearly.\n",
    "# - Forward/back fill edges to avoid NaNs before rounding Incidents.\n",
    "\n",
    "quarterly_list = []\n",
    "grp_cols = [\"SA2_CODE_2021\",\"metric\"]\n",
    "name_in = \"SA2_NAME_2021\" in yearly_all.columns\n",
    "\n",
    "if name_in:\n",
    "    grp_cols = [\"SA2_CODE_2021\",\"SA2_NAME_2021\",\"metric\"]\n",
    "\n",
    "for keys, g in yearly_all.groupby(grp_cols):\n",
    "    g = g.sort_values(\"Year\")\n",
    "    start_year = int(g[\"Year\"].min())\n",
    "    end_year   = END_YEAR\n",
    "\n",
    "    # yearly values at Dec-31\n",
    "    ts = pd.Series(g[\"value\"].values,\n",
    "                   index=pd.PeriodIndex(g[\"Year\"].astype(str), freq=\"A-DEC\").to_timestamp(how=\"end\")).sort_index()\n",
    "\n",
    "    # full quarterly index\n",
    "    qidx = pd.period_range(f\"{start_year}Q1\", f\"{end_year}Q4\", freq=\"Q-DEC\").to_timestamp(how=\"end\")\n",
    "    q = ts.reindex(qidx)\n",
    "    q_interp = q.interpolate(method=\"time\").ffill().bfill()  # ensure no NaNs remain\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"SA2_CODE_2021\": g[\"SA2_CODE_2021\"].iloc[0],\n",
    "        \"metric\": g[\"metric\"].iloc[0],\n",
    "        \"QuarterEnd\": q_interp.index,\n",
    "        \"Year\": q_interp.index.year,\n",
    "        \"Quarter\": q_interp.index.to_period(\"Q-DEC\").strftime(\"Q%q\"),\n",
    "        \"value\": q_interp.values\n",
    "    })\n",
    "    if name_in:\n",
    "        out[\"SA2_NAME_2021\"] = g[\"SA2_NAME_2021\"].iloc[0]\n",
    "\n",
    "    # keep non-negative for Incidents\n",
    "    if out[\"metric\"].iloc[0] == \"Incidents\":\n",
    "        out[\"value\"] = out[\"value\"].clip(lower=0)\n",
    "\n",
    "    quarterly_list.append(out)\n",
    "\n",
    "quarterly_all = pd.concat(quarterly_list, ignore_index=True)\n",
    "\n",
    "# Round Incidents safely (no NaNs thanks to ffill/bfill)\n",
    "quarterly_save = quarterly_all.copy()\n",
    "mask_q_inc = quarterly_save[\"metric\"] == \"Incidents\"\n",
    "quarterly_save.loc[mask_q_inc, \"value\"] = np.round(quarterly_save.loc[mask_q_inc, \"value\"]).astype(int)\n",
    "\n",
    "# Pivot to wide and save\n",
    "q_index = [\"SA2_CODE_2021\",\"Year\",\"Quarter\",\"QuarterEnd\"]\n",
    "if name_in:\n",
    "    q_index = [\"SA2_CODE_2021\",\"SA2_NAME_2021\",\"Year\",\"Quarter\",\"QuarterEnd\"]\n",
    "\n",
    "quarterly_wide = (\n",
    "    quarterly_save\n",
    "    .pivot_table(index=q_index, columns=\"metric\", values=\"value\", aggfunc=\"first\")\n",
    "    .reset_index()\n",
    "    .sort_values([\"SA2_CODE_2021\",\"Year\",\"Quarter\"])\n",
    ")\n",
    "\n",
    "ordered_cols_q = [c for c in [\"SA2_CODE_2021\",\"SA2_NAME_2021\",\"Year\",\"Quarter\",\"QuarterEnd\",\"Incidents\",\"CrimeRate\",\"VictimRate\"] if c in quarterly_wide.columns]\n",
    "quarterly_wide = quarterly_wide[ordered_cols_q + [c for c in quarterly_wide.columns if c not in ordered_cols_q]]\n",
    "\n",
    "quarterly_wide.to_csv(QUARTERLY_OUT, index=False)\n",
    "print(f\"[OK] Quarterly panel saved -> {QUARTERLY_OUT}\")\n",
    "\n",
    "# %% --------------------------------- QUICK SUMMARY ------------------------------------\n",
    "print(\"\\nSAMPLES (Yearly):\")\n",
    "print(yearly_wide.head(8).to_string(index=False))\n",
    "print(\"\\nSAMPLES (Quarterly):\")\n",
    "print(quarterly_wide.head(12).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89f7334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Yearly WIDE matrix -> /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_predictions_yearly_to_2030_WIDE_MATRIX.csv\n",
      "[OK] Quarterly WIDE matrix -> /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_predictions_quarterly_to_2030_WIDE_MATRIX.csv\n",
      "\n",
      "Yearly wide columns (sample): ['SA2_CODE_2021', 'SA2_NAME_2021', 'Incidents_2016', 'Incidents_2017', 'Incidents_2018', 'Incidents_2019', 'Incidents_2020', 'Incidents_2021', 'Incidents_2022', 'Incidents_2023', 'Incidents_2024', 'Incidents_2025', 'Incidents_2026', 'Incidents_2027', 'Incidents_2028'] ...\n",
      "Quarterly wide columns (sample): ['SA2_CODE_2021', 'SA2_NAME_2021', 'Incidents_2016Q1', 'Incidents_2016Q2', 'Incidents_2016Q3', 'Incidents_2016Q4', 'Incidents_2017Q1', 'Incidents_2017Q2', 'Incidents_2017Q3', 'Incidents_2017Q4', 'Incidents_2018Q1', 'Incidents_2018Q2', 'Incidents_2018Q3', 'Incidents_2018Q4', 'Incidents_2019Q1'] ...\n"
     ]
    }
   ],
   "source": [
    "# %% --------- Make YEARLY and QUARTERLY outputs \"wide matrix\" (one col per time slice) ---------\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# match your earlier paths\n",
    "IN_PATH = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_dataset_weighted_to_SA2.csv\"\n",
    "OUT_DIR = os.path.dirname(IN_PATH)\n",
    "YEARLY_IN  = os.path.join(OUT_DIR, \"crime_predictions_yearly_to_2030.csv\")\n",
    "QUARTERLY_IN = os.path.join(OUT_DIR, \"crime_predictions_quarterly_to_2030.csv\")\n",
    "\n",
    "YEARLY_WIDE_MATRIX_OUT   = os.path.join(OUT_DIR, \"crime_predictions_yearly_to_2030_WIDE_MATRIX.csv\")\n",
    "QUARTERLY_WIDE_MATRIX_OUT = os.path.join(OUT_DIR, \"crime_predictions_quarterly_to_2030_WIDE_MATRIX.csv\")\n",
    "\n",
    "# --- Load sources (from memory if present, else from disk) ---\n",
    "def _ensure_yearly_df():\n",
    "    try:\n",
    "        return yearly_wide.copy()\n",
    "    except NameError:\n",
    "        return pd.read_csv(YEARLY_IN)\n",
    "\n",
    "def _ensure_quarterly_df():\n",
    "    try:\n",
    "        return quarterly_wide.copy()\n",
    "    except NameError:\n",
    "        return pd.read_csv(QUARTERLY_IN)\n",
    "\n",
    "y_src = _ensure_yearly_df()\n",
    "q_src = _ensure_quarterly_df()\n",
    "\n",
    "# Detect id columns\n",
    "id_cols_year = [c for c in (\"SA2_CODE_2021\",\"SA2_NAME_2021\") if c in y_src.columns]\n",
    "id_cols_quart = [c for c in (\"SA2_CODE_2021\",\"SA2_NAME_2021\") if c in q_src.columns]\n",
    "\n",
    "# ------------------ YEARLY -> WIDE MATRIX: metric_year columns ------------------\n",
    "# If already long-like, keep; if \"tidy wide\", melt back to long\n",
    "y_metrics = [c for c in [\"Incidents\",\"CrimeRate\",\"VictimRate\"] if c in y_src.columns]\n",
    "if y_metrics:\n",
    "    y_long = y_src.melt(id_vars=id_cols_year+[\"Year\"], value_vars=y_metrics,\n",
    "                        var_name=\"metric\", value_name=\"value\").dropna(subset=[\"value\"])\n",
    "else:\n",
    "    # assume already long with columns: id, Year, metric, value\n",
    "    y_long = y_src.rename(columns={\"value\":\"value\", \"metric\":\"metric\"})\n",
    "\n",
    "y_long[\"colname\"] = y_long[\"metric\"] + \"_\" + y_long[\"Year\"].astype(int).astype(str)\n",
    "\n",
    "yearly_matrix = (\n",
    "    y_long\n",
    "    .pivot_table(index=id_cols_year, columns=\"colname\", values=\"value\", aggfunc=\"first\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# sort columns nicely (Incidents, CrimeRate, VictimRate; chronological)\n",
    "fixed = id_cols_year\n",
    "metric_order = [\"Incidents\",\"CrimeRate\",\"VictimRate\"]\n",
    "time_cols = [c for c in yearly_matrix.columns if c not in fixed]\n",
    "def _metric_key(c):\n",
    "    for i,m in enumerate(metric_order):\n",
    "        if c.startswith(m+\"_\"):\n",
    "            return (i, int(c.split(\"_\")[1]))\n",
    "    return (99, 9999)\n",
    "time_cols_sorted = sorted(time_cols, key=_metric_key)\n",
    "yearly_matrix = yearly_matrix[fixed + time_cols_sorted]\n",
    "\n",
    "yearly_matrix.to_csv(YEARLY_WIDE_MATRIX_OUT, index=False)\n",
    "print(f\"[OK] Yearly WIDE matrix -> {YEARLY_WIDE_MATRIX_OUT}\")\n",
    "\n",
    "# ---------------- QUARTERLY -> WIDE MATRIX: metric_YYYYQn columns ----------------\n",
    "# If tidy wide, melt back; else assume long\n",
    "q_metrics = [c for c in [\"Incidents\",\"CrimeRate\",\"VictimRate\"] if c in q_src.columns]\n",
    "if q_metrics:\n",
    "    q_long = q_src.melt(id_vars=id_cols_quart+[\"Year\",\"Quarter\",\"QuarterEnd\"],\n",
    "                        value_vars=q_metrics, var_name=\"metric\", value_name=\"value\").dropna(subset=[\"value\"])\n",
    "else:\n",
    "    q_long = q_src.rename(columns={\"value\":\"value\", \"metric\":\"metric\"})\n",
    "\n",
    "# Build period label like 2027Q3\n",
    "# Quarter might be \"Q1\" already; ensure it's clean\n",
    "q_long[\"Quarter\"] = q_long[\"Quarter\"].astype(str).str.replace(r\"[^Q\\d]\", \"\", regex=True)\n",
    "q_long[\"Period\"] = q_long[\"Year\"].astype(int).astype(str) + q_long[\"Quarter\"]\n",
    "\n",
    "q_long[\"colname\"] = q_long[\"metric\"] + \"_\" + q_long[\"Period\"]\n",
    "\n",
    "quarterly_matrix = (\n",
    "    q_long\n",
    "    .pivot_table(index=id_cols_quart, columns=\"colname\", values=\"value\", aggfunc=\"first\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# sort quarterly columns (metric order, then year, then Q1..Q4)\n",
    "fixed_q = id_cols_quart\n",
    "q_time_cols = [c for c in quarterly_matrix.columns if c not in fixed_q]\n",
    "\n",
    "def _q_key(c):\n",
    "    # c like \"Incidents_2028Q3\"\n",
    "    try:\n",
    "        m, rest = c.split(\"_\", 1)\n",
    "        year = int(rest[:-2])  # drop 'Qx'\n",
    "        qn = int(rest[-1:])\n",
    "        mpos = metric_order.index(m) if m in metric_order else 99\n",
    "        return (mpos, year, qn)\n",
    "    except Exception:\n",
    "        return (99, 9999, 9)\n",
    "\n",
    "q_time_cols_sorted = sorted(q_time_cols, key=_q_key)\n",
    "quarterly_matrix = quarterly_matrix[fixed_q + q_time_cols_sorted]\n",
    "\n",
    "quarterly_matrix.to_csv(QUARTERLY_WIDE_MATRIX_OUT, index=False)\n",
    "print(f\"[OK] Quarterly WIDE matrix -> {QUARTERLY_WIDE_MATRIX_OUT}\")\n",
    "\n",
    "# Quick peek\n",
    "print(\"\\nYearly wide columns (sample):\", yearly_matrix.columns[:15].tolist(), \"...\")\n",
    "print(\"Quarterly wide columns (sample):\", quarterly_matrix.columns[:15].tolist(), \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5390d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd8b866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] File saved with QuarterDate (first day of ending month):\n",
      "/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_predictions_quarterly_to_2030_datetime.csv\n",
      "\n",
      "Preview:\n",
      " SA2_CODE_2021 SA2_NAME_2021  Year Quarter QuarterDate  Incidents   CrimeRate  VictimRate\n",
      "     201011001     Alfredton  2016      Q1  2016-03-01    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2016      Q2  2016-06-01    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2016      Q3  2016-09-01    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2016      Q4  2016-12-01    14606.0 8723.671498 5740.096618\n",
      "     201011001     Alfredton  2017      Q1  2017-03-01    14680.0 8725.324856 5631.765829\n",
      "     201011001     Alfredton  2017      Q2  2017-06-01    14755.0 8726.996585 5522.231365\n",
      "     201011001     Alfredton  2017      Q3  2017-09-01    14831.0 8728.686685 5411.493225\n",
      "     201011001     Alfredton  2017      Q4  2017-12-01    14907.0 8730.376785 5300.755085\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Path setup ---\n",
    "IN_PATH = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_predictions_quarterly_to_2030.csv\"\n",
    "OUT_PATH = IN_PATH.replace(\".csv\", \"_datetime.csv\")\n",
    "\n",
    "# --- Load dataset ---\n",
    "df = pd.read_csv(IN_PATH)\n",
    "\n",
    "# Check that required columns exist\n",
    "required_cols = [\"Year\", \"Quarter\"]\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}. Found: {df.columns.tolist()}\")\n",
    "\n",
    "# --- Normalize Quarter column ---\n",
    "df[\"Quarter\"] = df[\"Quarter\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# --- Map quarters to ending months (Mar, Jun, Sep, Dec) ---\n",
    "quarter_to_month = {\"Q1\": 3, \"Q2\": 6, \"Q3\": 9, \"Q4\": 12}\n",
    "\n",
    "# --- Create QuarterDate = first day of ending month ---\n",
    "df[\"QuarterDate\"] = pd.to_datetime(\n",
    "    df[\"Year\"].astype(str) + \"-\" + df[\"Quarter\"].map(quarter_to_month).astype(str) + \"-01\"\n",
    ")\n",
    "\n",
    "# --- Drop old QuarterEnd column if present ---\n",
    "if \"QuarterEnd\" in df.columns:\n",
    "    df = df.drop(columns=[\"QuarterEnd\"])\n",
    "\n",
    "# --- Reorder columns neatly ---\n",
    "cols_order = [\"SA2_CODE_2021\", \"SA2_NAME_2021\", \"Year\", \"Quarter\", \"QuarterDate\", \n",
    "              \"Incidents\", \"CrimeRate\", \"VictimRate\"]\n",
    "df = df[[c for c in cols_order if c in df.columns] + [c for c in df.columns if c not in cols_order]]\n",
    "\n",
    "# --- Save updated file ---\n",
    "df.to_csv(OUT_PATH, index=False)\n",
    "print(f\"[OK] File saved with QuarterDate (first day of ending month):\\n{OUT_PATH}\")\n",
    "\n",
    "# --- Preview few rows ---\n",
    "print(\"\\nPreview:\")\n",
    "print(df.head(8).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1b446",
   "metadata": {},
   "source": [
    "## JOINING DATASET with Prediction_df ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec6093c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Merged file saved to:\n",
      "/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/prediction_with_crime.csv\n",
      "\n",
      "Preview:\n",
      "       Lat        Lng  SA2_CODE_2021 SA2_NAME21   Suburb       date          ERP  Predicted_Income  t SA2_NAME_2021  Year Quarter  Incidents   CrimeRate  VictimRate\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Paths ---\n",
    "crime_path = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/crime_predictions_quarterly_to_2030_datetime.csv\"\n",
    "pred_path  = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/predicition-data/prediction_df.csv\"\n",
    "merged_out = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/predicition-data/prediction_with_crime.csv\"\n",
    "\n",
    "# --- Load datasets ---\n",
    "crime_df = pd.read_csv(crime_path)\n",
    "pred_df  = pd.read_csv(pred_path)\n",
    "\n",
    "# --- Standardize SA2 column names ---\n",
    "for df in [crime_df, pred_df]:\n",
    "    if \"SA2_CODE_2021\" not in df.columns:\n",
    "        alt = [c for c in df.columns if \"SA2\" in c and \"CODE\" in c]\n",
    "        if alt:\n",
    "            df.rename(columns={alt[0]: \"SA2_CODE_2021\"}, inplace=True)\n",
    "\n",
    "# --- Convert to datetime ---\n",
    "crime_df[\"QuarterDate\"] = pd.to_datetime(crime_df[\"QuarterDate\"])\n",
    "pred_df[\"date\"] = pd.to_datetime(pred_df[\"date\"])\n",
    "\n",
    "# --- Optional: sort for clarity ---\n",
    "crime_df = crime_df.sort_values([\"SA2_CODE_2021\",\"QuarterDate\"])\n",
    "pred_df = pred_df.sort_values([\"SA2_CODE_2021\",\"date\"])\n",
    "\n",
    "# --- Merge on SA2 code and matching date ---\n",
    "merged = pred_df.merge(\n",
    "    crime_df,\n",
    "    how=\"left\",\n",
    "    left_on=[\"SA2_CODE_2021\",\"date\"],\n",
    "    right_on=[\"SA2_CODE_2021\",\"QuarterDate\"]\n",
    ")\n",
    "\n",
    "# --- Drop duplicate key columns (if you want to keep only one datetime column) ---\n",
    "merged = merged.drop(columns=[\"QuarterDate\"])\n",
    "\n",
    "# --- Save merged dataset ---\n",
    "out_path = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/prediction_with_crime.csv\"\n",
    "merged.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"[OK] Merged file saved to:\\n{out_path}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(merged.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c074b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Duplicates removed and saved -> /Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/prediction_with_crime_deduped.csv\n",
      "Rows: 3,197 | Columns: 15\n",
      "\n",
      "Sample:\n",
      "       Lat        Lng  SA2_CODE_2021 SA2_NAME21   Suburb       date          ERP  Predicted_Income  t SA2_NAME_2021  Year Quarter  Incidents   CrimeRate  VictimRate\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-06-01 11720.193827      59796.465049 33      Ballarat  2025      Q2    15728.0 7938.753559 4279.657264\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-09-01 11710.778773      60156.514521 34      Ballarat  2025      Q3    16075.0 8068.863846 4262.661917\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2025-12-01 11701.466056      60512.650411 35      Ballarat  2025      Q4    16422.0 8198.974132 4245.666569\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2026-03-01 11701.692542      60864.872720 36      Ballarat  2026      Q1    15875.0 7880.347396 4064.626192\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2026-06-01 11706.992600      61224.922192 37      Ballarat  2026      Q2    15321.0 7558.180363 3881.574254\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2026-09-01 11712.292657      61584.971663 38      Ballarat  2026      Q3    14762.0 7232.473033 3696.510757\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2026-12-01 11717.535106      61941.107554 39      Ballarat  2026      Q4    14202.0 6906.765703 3511.447260\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2027-03-01 11722.719945      62293.329863 40      Ballarat  2027      Q1    14212.0 6872.851262 3468.083021\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2027-06-01 11728.020002      62653.379335 41      Ballarat  2027      Q2    14222.0 6838.559993 3424.236957\n",
      "-37.556144 143.836655      201011002   Ballarat Ballarat 2027-09-01 11733.320060      63013.428806 42      Ballarat  2027      Q3    14232.0 6803.891897 3379.909068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Path to your merged dataset ---\n",
    "IN_PATH = \"/Users/rajaa/Desktop/Applied Data Science MAST30034/project-2-group-real-estate-industry-project-7-2025/Raja-workspace/notebooks/landing/raw/cleaned/3. crime/prediction_with_crime.csv\"\n",
    "OUT_PATH = IN_PATH.replace(\".csv\", \"_deduped.csv\")\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(IN_PATH)\n",
    "\n",
    "# --- Drop duplicate columns (same name) ---\n",
    "df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n",
    "\n",
    "# --- Drop duplicate rows (entirely identical) ---\n",
    "df = df.drop_duplicates(keep=\"first\")\n",
    "\n",
    "# --- Optional: reset index ---\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# --- Save cleaned dataset ---\n",
    "df.to_csv(OUT_PATH, index=False)\n",
    "print(f\"[OK] Duplicates removed and saved -> {OUT_PATH}\")\n",
    "\n",
    "# --- Preview summary ---\n",
    "print(f\"Rows: {len(df):,} | Columns: {len(df.columns)}\")\n",
    "print(\"\\nSample:\")\n",
    "print(df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a057dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
